{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b0f965",
   "metadata": {},
   "source": [
    "# Here we construct KG from the data we have created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1418ff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries here\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import sklearn\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5bcaa0",
   "metadata": {},
   "source": [
    "# Create the input data structure by reading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a0689f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Prasad\\MSIITM\\Research\\Dataset\\train\\pre_req_dataset\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b52988e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15259\n"
     ]
    }
   ],
   "source": [
    "triplets = pd.read_csv('TrainingData.csv')\n",
    "def process_triplets(number_of_triplets):\n",
    "    # Initializing the dictionary to store the data\n",
    "    triplets = number_of_triplets.sample(frac=1)\n",
    "    data = []\n",
    "    nodes = set(number_of_triplets['Head']) \n",
    "    nodes.update(set(number_of_triplets['Tail']))\n",
    "    relations = set(number_of_triplets['Relation'])\n",
    "    for index, row in triplets.iterrows():\n",
    "        tmp = (row['Head'],row['Tail'],row['Relation'])\n",
    "        data.append(tmp)\n",
    "    return data, nodes, relations\n",
    "\n",
    "# Process the triplets from the file\n",
    "triplet_data, nodes, relations = process_triplets(triplets)\n",
    "\n",
    "# Displaying a small portion of the data to verify correctness\n",
    "print(len(triplet_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48447675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_data(data, train_percent=0.6, validation_percent=0.2, test_percent=0.2):\n",
    "    # Shuffle the data to ensure random distribution\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # Calculate the number of data points for each set\n",
    "    total_data_points = len(data)\n",
    "    train_size = int(total_data_points * train_percent)\n",
    "    validation_size = int(total_data_points * validation_percent)\n",
    "\n",
    "    # Split the data into training, validation, and test sets\n",
    "    train_data = data[:train_size]\n",
    "    validation_data = data[train_size:train_size + validation_size]\n",
    "    test_data = data[train_size + validation_size:]\n",
    "\n",
    "    return train_data, validation_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b26ac279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# A function to convert our data into required format\n",
    "def dataconv(data):\n",
    "    # Extracting the columns\n",
    "    heads = [row[0] for row in data]\n",
    "    tails = [row[1] for row in data]\n",
    "    relations = [row[2] for row in data]\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    head_tensor = torch.tensor(heads, dtype=torch.long)\n",
    "    tail_tensor = torch.tensor(tails, dtype=torch.long)\n",
    "    relation_tensor = torch.tensor(relations, dtype=torch.long)\n",
    "\n",
    "    # Store in a dictionary\n",
    "    graph_data = {\n",
    "        'edge_index': torch.stack((head_tensor, tail_tensor), dim=0),\n",
    "        'edge_reltype': relation_tensor,\n",
    "        'num_nodes': 1268\n",
    "    }\n",
    "    return graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45ee3cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set  = split_data(triplet_data)\n",
    "train_set = dataconv(train_set)\n",
    "val_set = dataconv(val_set)\n",
    "test_set = dataconv(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fe4d117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3 3\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set),len(val_set), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36fbfd71",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_set\u001b[38;5;241m.\u001b[39msize()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "train_set.size()\n",
    "# torch.randint(1269, 5 + 1269 , (1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7d1f9f",
   "metadata": {},
   "source": [
    "# Now Dataset class and create negative edge for positive edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28a42e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class RelationDataset(Dataset):\n",
    "  def __init__(self, true_edges, filter=False):\n",
    "    self.true_edges = true_edges\n",
    "#     self.train_edges = edges\n",
    "    self.edge_index = true_edges['edge_index']\n",
    "    self.edge_reltype = true_edges['edge_reltype']\n",
    "    self.num_nodes = true_edges['num_nodes']\n",
    "    self.num_rels = 5\n",
    "    self.rel_dict = {}\n",
    "    self.true_edge_dict = {}\n",
    "    self.filter = filter\n",
    "\n",
    "    # We construct a dictionary that maps edges to relation types\n",
    "    # We do this to quickly filter out postive edges while sampling negative edges.\n",
    "    for i in range(self.true_edges['edge_index'].shape[1]):\n",
    "      h = self.true_edges['edge_index'][0, i]\n",
    "      t = self.true_edges['edge_index'][1, i]\n",
    "      r = self.true_edges['edge_reltype'][i]\n",
    "      if (h,t) not in self.true_edge_dict:\n",
    "        self.true_edge_dict[(h,t)] = []\n",
    "      self.true_edge_dict[(h,t)].append(r)\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.edge_index.size(1)\n",
    "\n",
    "  def _sample_negative_edge(self, idx):\n",
    "    sample = random.uniform(0, 1)\n",
    "    found = False\n",
    "    while not found:\n",
    "      if sample <= 0.4:\n",
    "        # corrupt the head entity\n",
    "        h = self.edge_index[0, idx]\n",
    "        t = torch.randint(0, self.num_nodes, (1,))\n",
    "        r = self.edge_reltype[idx]\n",
    "      elif 0.4 < sample < 0.8:\n",
    "        # corrupt the tail entity\n",
    "        t = self.edge_index[1, idx]\n",
    "        h = torch.randint(0, self.num_nodes, (1,))\n",
    "        r = self.edge_reltype[idx]\n",
    "      else:\n",
    "        # corrupt the relation\n",
    "        # adding this auxilliary loss is shown to improve performance\n",
    "        t = self.edge_index[1, idx]\n",
    "        h = self.edge_index[0, idx]\n",
    "        r = torch.randint(1269, self.num_rels + 1269 , (1,))\n",
    "      if not self.filter:\n",
    "        found = True\n",
    "      else:\n",
    "        # check if the edge is a true edge\n",
    "        if (h, t) not in self.true_edge_dict:\n",
    "          found = True\n",
    "        elif r not in self.true_edge_dict[(h, t)]:\n",
    "          found = True\n",
    "    data = [torch.tensor([h,t]), r]\n",
    "    return data\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    pos_sample = [self.edge_index[:, idx], self.edge_reltype[idx]]\n",
    "    neg_sample = self._sample_negative_edge(idx)\n",
    "    return pos_sample, neg_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6712b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestRelationDataset(Dataset):\n",
    "    def __init__(self, true_edges, filter=False, num_neg=14000, mode='head'):\n",
    "        self.true_edges = true_edges\n",
    "        #     self.train_edges = edges\n",
    "        self.edge_index = true_edges['edge_index']\n",
    "        self.edge_reltype = true_edges['edge_reltype']\n",
    "        self.num_nodes = true_edges['num_nodes']\n",
    "        self.num_rels = 5\n",
    "        self.rel_dict = {}\n",
    "        self.true_edge_dict = {}\n",
    "        self.filter = filter\n",
    "\n",
    "        # We construct a dictionary that maps edges to relation types\n",
    "        # We do this to quickly filter out postive edges while sampling negative edges.\n",
    "        for i in range(self.true_edges['edge_index'].shape[1]):\n",
    "            h = self.true_edges['edge_index'][0, i]\n",
    "            t = self.true_edges['edge_index'][1, i]\n",
    "            r = self.true_edges['edge_reltype'][i]\n",
    "            if (h,t) not in self.true_edge_dict:\n",
    "                self.true_edge_dict[(h,t)] = []\n",
    "            self.true_edge_dict[(h,t)].append(r)\n",
    "    \n",
    "    def __len__(self):\n",
    "         return self.edge_index.size(1)\n",
    "\n",
    "    def _sample_negative_edge(self, idx, mode):\n",
    "        triples = []\n",
    "        random_node_idx = list(range(self.num_nodes))\n",
    "        random.shuffle(random_node_idx)\n",
    "        for n in random_node_idx:\n",
    "            r = self.edge_reltype[idx]\n",
    "            if mode == 'head':\n",
    "                # corrupt tail if in head mode\n",
    "                t = torch.tensor(n)\n",
    "                h = self.edge_index[0, idx]\n",
    "            elif mode == 'tail':\n",
    "                # corrupt head if in tail mode\n",
    "                h = torch.tensor(n)\n",
    "                t = self.edge_index[1, idx]\n",
    "            ht = torch.tensor([h, t])\n",
    "            if self.filter:\n",
    "                # check if edge is present in the knowledge graph\n",
    "                if (h, t) not in self.true_edge_dict:\n",
    "                    triples.append([ht, r])\n",
    "                elif r not in self.true_edge_dict[(h, t)]:\n",
    "                    triples.append([ht, r])\n",
    "            else:\n",
    "                triples.append([ht, r])\n",
    "            if len(triples) == self.num_neg:\n",
    "                break\n",
    "        return triples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pos_sample = [self.edge_index[:, idx], self.edge_reltype[idx]]\n",
    "        neg_samples = self._sample_negative_edge(idx, mode=self.mode)\n",
    "        edges = torch.stack([pos_sample[0]] + [ht for ht, _ in neg_samples])\n",
    "        edge_reltype = torch.stack([pos_sample[1]] + [r for _, r in neg_samples])\n",
    "        return edges, edge_reltype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1306c900",
   "metadata": {},
   "source": [
    "# Let's define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e05e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransE(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim):\n",
    "        super(TransE, self).__init__()\n",
    "        self.entity_embeddings = nn.Parameter(torch.randn(num_entities, embedding_dim))\n",
    "        self.relation_embeddings = nn.Parameter(torch.randn(num_relations, embedding_dim))\n",
    "    \n",
    "    def forward(self):\n",
    "        self.entity_embeddings.data[:-1,:].div_(\n",
    "        self.entity_embeddings.data[:-1,:].norm(p=2, dim=1, keepdim=True))\n",
    "        return self.entity_embeddings, self.relation_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b86838c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransE_loss(pos_edges, neg_edges, pos_reltype, neg_reltype, entity_embeddings, relation_embeddings):\n",
    "    # Select embeddings for both positive and negative samples\n",
    "    pos_head_embeds = torch.index_select(entity_embeddings, 0, pos_edges[:, 0])\n",
    "    pos_tail_embeds = torch.index_select(entity_embeddings, 0, pos_edges[:, 1])\n",
    "    neg_head_embeds = torch.index_select(entity_embeddings, 0, neg_edges[:, 0])\n",
    "    neg_tail_embeds = torch.index_select(entity_embeddings, 0, neg_edges[:, 1])\n",
    "    pos_relation_embeds = torch.index_select(relation_embeddings, 0, pos_reltype.squeeze())\n",
    "    neg_relation_embeds = torch.index_select(relation_embeddings, 0, neg_reltype.squeeze())\n",
    "\n",
    "    # Calculate the distance score\n",
    "    d_pos = torch.norm(pos_head_embeds + pos_relation_embeds - pos_tail_embeds, p=1, dim=1)\n",
    "    d_neg = torch.norm(neg_head_embeds + neg_relation_embeds - neg_tail_embeds, p=1, dim=1)\n",
    "    ones = torch.ones(d_pos.size(0))\n",
    "\n",
    "    # margin loss - we want to increase d_neg and decrease d_pos\n",
    "    margin_loss = torch.nn.MarginRankingLoss(margin=1.)\n",
    "    loss = margin_loss(d_neg, d_pos, ones)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc1aaa3",
   "metadata": {},
   "source": [
    "# Let's create evaluation metric and evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "867d7e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(y_pred):\n",
    "    argsort = torch.argsort(y_pred, dim = 1, descending = False)\n",
    "    # not using argsort to do the rankings to avoid bias when the scores are equal\n",
    "    ranking_list = torch.nonzero(argsort == 0, as_tuple=False)\n",
    "    ranking_list = ranking_list[:, 1] + 1\n",
    "    hits1_list = (ranking_list <= 1).to(torch.float)\n",
    "    hits3_list = (ranking_list <= 3).to(torch.float)\n",
    "    hits10_list = (ranking_list <= 10).to(torch.float)\n",
    "    mr_list = ranking_list.to(torch.float)\n",
    "    mrr_list = 1./ranking_list.to(torch.float)\n",
    "    return hits1_list.mean(), hits3_list.mean(), hits10_list.mean(), mr_list.mean(), mrr_list.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b40f9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(entity_embeddings, relation_embeddings, dataloader, kg_model = \"TransE\", iters=None, gamma = 5.0, epsilon = 2.0):\n",
    "    hits1_list = []\n",
    "    hits3_list = []\n",
    "    hits10_list = []\n",
    "    mr_list = []\n",
    "    mrr_list = []\n",
    "    data_iterator = iter(dataloader)\n",
    "    if iters is None:\n",
    "        iters = len(dataloader)\n",
    "    for _ in tqdm.trange(iters, desc=\"Evaluating\"):\n",
    "        batch = next(data_iterator)\n",
    "        edges, edge_reltype = batch\n",
    "        b, num_samples, _= edges.size()\n",
    "        edges = edges.view(b*num_samples, -1)\n",
    "        edge_reltype = edge_reltype.view(b*num_samples, -1)\n",
    "\n",
    "        head_embeds = torch.index_select(entity_embeddings, 0, edges[:, 0])\n",
    "        relation_embeds = torch.index_select(relation_embeddings, 0, edge_reltype.squeeze())\n",
    "        tail_embeds = torch.index_select(entity_embeddings, 0, edges[:, 1])\n",
    "\n",
    "        if kg_model == \"TransE\":\n",
    "            scores = torch.norm(head_embeds + relation_embeds - tail_embeds, p=1, dim=1)\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported model {kg_model}')\n",
    "\n",
    "        scores = scores.view(b, num_samples)\n",
    "    \n",
    "        hits1, hits3, hits10, mr, mrr = eval_metrics(scores)\n",
    "        hits1_list.append(hits1.item())\n",
    "        hits3_list.append(hits3.item())\n",
    "        hits10_list.append(hits10.item())\n",
    "        mr_list.append(mr.item())\n",
    "        mrr_list.append(mrr.item()) \n",
    "\n",
    "    hits1 = sum(hits1_list)/len(hits1_list)\n",
    "    hits3 = sum(hits3_list)/len(hits1_list)\n",
    "    hits10 = sum(hits10_list)/len(hits1_list)\n",
    "    mr = sum(mr_list)/len(hits1_list)\n",
    "    mrr = sum(mrr_list)/len(hits1_list)\n",
    "\n",
    "    return hits1, hits3, hits10, mr, mrr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140d58d3",
   "metadata": {},
   "source": [
    "# Let's create the model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80fcebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_entities = 1268\n",
    "num_relations = 5\n",
    "embedding_dim = 100\n",
    "\n",
    "kg_model = \"TransE\" \n",
    "epochs = 100 \n",
    "batch_size = 128 \n",
    "learning_rate = 1e-3 \n",
    "\n",
    "model = TransE(num_entities, num_relations, embedding_dim)\n",
    "model_loss = TransE_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fdec10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.4684,  0.2276,  0.1232,  ..., -0.8076,  1.1650, -0.8651],\n",
       "        [ 0.5769, -0.7604, -0.1334,  ..., -1.3173,  0.0216, -0.3910],\n",
       "        [-1.1355, -0.1643,  0.2844,  ..., -0.1874, -1.2541, -1.0203],\n",
       "        ...,\n",
       "        [-1.5833, -1.1031,  1.1048,  ..., -1.5604, -0.0812, -0.8178],\n",
       "        [ 0.8155,  0.2081,  0.9863,  ...,  0.0092,  2.2581, -2.1244],\n",
       "        [-0.1007, -0.3710, -0.1092,  ...,  0.8020,  0.0508, -0.5738]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.entity_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71afab4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size 9155\n",
      "Val dataset size 3051\n",
      "Val dataset size 3051\n",
      "Val dataset size 3053\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "num_workers = os.cpu_count()\n",
    "\n",
    "train_dataset = RelationDataset(train_set, filter = True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size= batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_dataset = RelationDataset(val_set, filter = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size= batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "val_eval_dataset = TestRelationDataset(val_set, filter = True, num_neg= 100)\n",
    "val_eval_dataloader = DataLoader(val_eval_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_dataset = TestRelationDataset(test_set, filter=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "print(f'Train dataset size {len(train_dataset)}')\n",
    "print(f'Val dataset size {len(val_dataset)}')\n",
    "print(f'Val dataset size {len(val_eval_dataset)}')\n",
    "print(f'Val dataset size {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ae78cf",
   "metadata": {},
   "source": [
    "# Let's train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec94e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use adam optimizer for training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "for e in range(epochs):\n",
    "    losses = []\n",
    "    #check metric for every 10th epoch\n",
    "    if e%10 == 0:\n",
    "        model.eval()\n",
    "        h1, h3, h10, mr, mrr = eval(model.entity_embeddings, model.relation_embeddings, val_eval_dataloader, kg_model, iters=10)\n",
    "        print(f\"hits@1:{h1} hits@3:{h3} hits@10:{h10} mr:{mr} mrr:{mrr}\")\n",
    "    model.train()\n",
    "    for step, batch in enumerate(tqdm.tqdm(train_dataloader, desc=\"Training\")):\n",
    "        pos_sample, neg_sample = batch\n",
    "        entity_embedding_sample, relation_embedding_sample = model()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = model_loss(pos_sample[0], neg_sample[0], pos_sample[1], neg_sample[1], entity_embedding_sample, relation_embedding_sample)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    val_losses = []\n",
    "    model.eval()\n",
    "    entity_embedding_pass, relation_embedding_pass = model()\n",
    "    for step, batch in enumerate(tqdm.tqdm(val_dataloader, desc=\"Validation\")):\n",
    "        pos_sample, neg_sample = batch\n",
    "        loss = model_loss(pos_sample[0], neg_sample[0], pos_sample[1], neg_sample[1], entity_embedding_pass, relation_embedding_pass)\n",
    "        val_losses.append(loss.item())\n",
    "    print(f\"epoch: {e + 1} loss: {sum(losses)/len(losses)} val_loss: {sum(val_losses)/len(val_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62453d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = train_dataloader\n",
    "a[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
